{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4:\n",
    "<ol>\n",
    "<li>Import orders table from mysql as text file to the destination /user/cloudera/problem5/text. Fields should be terminated by a tab character (\"\\t\") character and lines should be terminated by new line character (\"\\n\"). </li>\n",
    "<li>Import orders table from mysql  into hdfs to the destination /user/cloudera/problem5/avro. File should be stored as avro file.</li>\n",
    "<li>Import orders table from mysql  into hdfs  to folders /user/cloudera/problem5/parquet. File should be stored as parquet file.</li>\n",
    "    \n",
    "<br>  \n",
    "<font color=red>**I need to figure out the answer for the both questions as it deals with Sqoop and Flume**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transform/Convert data-files at /user/cloudera/problem5/avro and store the converted file at the following locations and file formats\n",
    "<ol>\n",
    "<li>save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem5/parquet-snappy-compress</li>\n",
    "<li>save the data to hdfs using gzip compression as text file at /user/cloudera/problem5/text-gzip-compress</li>\n",
    "<li>save the data to hdfs using no compression as sequence file at /user/cloudera/problem5/sequence</li>\n",
    "<li>save the data to hdfs using snappy compression as text file at /user/cloudera/problem5/text-snappy-compress</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Let's first start the pyspark with databricks package for the avro file format</font>\n",
    "<br>  \n",
    "`pyspark --packages com.databricks:spark-avro_2.11:4.0.0`\n",
    "<br>  \n",
    "<font color=blue>The package picked was based on the Scala and Spark Version installed on your system</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Let's create a schema for Products Data</font>\n",
    "\n",
    "`from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType`\n",
    "\n",
    "`prod_schema = StructType([\n",
    " StructField(\"product_int\", IntegerType(), True), \\\n",
    " StructField(\"product_category_id\", IntegerType(), True), \\\n",
    " StructField(\"product_name\", StringType(), True), \\\n",
    " StructField(\"product_description\", StringType(), True), \\\n",
    " StructField(\"product_price\", FloatType(), True), \\\n",
    " StructField(\"product_image\", StringType(), True) ])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Now we can create a products dataframe with the new schema and the location from hdfs</font>\n",
    "\n",
    "`products_df = spark.read.format(\"csv\").schema(prod_schema).load(\"/user/retail_db/products/part-*\")`\n",
    "\n",
    "<font color=blue>Write the Dataframe to HDFS in Avro file format</font>\n",
    "<br>  \n",
    "`products_df.write.format(\"com.databrick.spark.avro\").option(\"mode, overwrite\").option(\"compresssion\",\"snappy\").save(\"/user/cca-problem/problem5/avro-snappy/\")`\n",
    "<br>  \n",
    "`products_avro = spark.read.format(\"com.databricks.spark.avro\").load(\"/user/cca-problem/problem5/avro-snappy/part-*\")`\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Lets validate if the file has been written to HDFS</font>\n",
    "\n",
    "`import os`\n",
    "`os.system(\"hdfs dfs -ls /user/cca-problem/problem5/avro-snappy/\")`\n",
    "<br>  \n",
    "<font color=blue>Create the Products Dataframe based on the AVRO format</font>\n",
    "`prod_avro = spark.read.format(\"com.databricks.spark.avro\").load(\"/user/cca-problem/problem5/avro-snappy/part-*\")`\n",
    "<br>  \n",
    "<font color=green>i. save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem5/parquet-snappy-compress</font>\n",
    "<br>  \n",
    "`prod_parq_snappy = prod_avro.write.format(\"parquet\").option(\"compression\", \"snappy\").save(\"/user/cca-problem/problem5/parquet-snappy-compress/\")`\n",
    "<br>  \n",
    "`prod_parquet = spark.read.format(\"parquet\").load(\"/user/cca-problem/problem5/parquet-snappy-compress/part-*\")`\n",
    "***\n",
    "<br>  \n",
    "<font color=green>ii. save the data to hdfs using gzip compression as text file at /user/cloudera/problem5/text-gzip-compress</font>\n",
    "<br>  \n",
    "`from pyspark.sql.functions import concat_ws`\n",
    "<br>  \n",
    "`codec = \"org.apache.hadoop.io.compression.GzipCodec\"`\n",
    "<br>  \n",
    "`prod_parq_snappy = prod_avro.select(concat_ws(\"\\t\",*prod_avro.columns)).rdd.map(lambda rec: (rec[0])).saveAsTextFile(\"/user/cca-problem/problem5/text-gzip-compress/\", codec)`\n",
    "<br>  \n",
    "<font color=green>iii. save the data to hdfs using no compression as sequence file at /user/cloudera/problem5/sequence</font>\n",
    "<br>  \n",
    "***\n",
    "<ol>\n",
    "<li>org.apache.hadoop.io.compress.DefaultCodec</li>\n",
    "<li>org.apache.hadoop.io.compress.GzipCodec</li>\n",
    "<li>org.apache.hadoop.io.compress.BZip2Codec</li>\n",
    "<li>org.apache.hadoop.io.compress.DeflateCodec</li>\n",
    "<li>org.apache.hadoop.io.compress.SnappyCodec</li>\n",
    "<li>org.apache.hadoop.io.compress.Lz4Codec</li>\n",
    "</ol>\n",
    "\n",
    "`prod_seq_nocompress = prod_avro.select(prod_avro[0]), concat_ws(\"\\t\", *prod_avro.columns)).rdd.map(lambda rec: (int(rec[0]), rec[1])).saveAsSequenceFile(\"/user/cca-problem/problem5/sequence/\")`\n",
    "<br>  \n",
    "<font color=green>It can be read by the below form:</font>\n",
    "\n",
    "`prod_sequence = sc.sequenceFile(\"/user/cca-problem/problem5/sequence/part-*\").toDf()`\n",
    "  \n",
    "***\n",
    "<font color=blue>Lets validate if the file has been write to HDFS in Parquet format</font>\n",
    "<br>  \n",
    "`os.system(\"hdfs dfs -ls /user/cca-problem/problem5/parquet-snappy/\")`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transform/Convert data-files at /user/cloudera/problem5/parquet-snappy-compress and store the converted file at the following locations and file formats\n",
    "<ol>\n",
    "<li>save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress</li>\n",
    "<li>save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>The below dataframe can be utilized to perform the next two tasks</font>\n",
    "<br>  \n",
    "`prod_parquet` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**1. save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress**</font>\n",
    "<br>  \n",
    "`prod_parquet.write.format(\"parquet\").option(\"compression\",\"None\").option(\"mode\",\"overwrite\").save(\"/user/cca-practice/problem5/parquet-no-compress/\")`\n",
    "<br>  \n",
    "<font color=green>**2. save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy**</font>\n",
    "<br>  \n",
    "`prod_parquet.write.format(\"com.databricks.spark.avro\").option(\"compression\",\"snappy\").option(\"mode\",\"overwrite\").save(\"/user/cca-practice/problem5/avro-snappy/\")`\n",
    "<br>  \n",
    "<font color=blue>Please be noted that the default compression for Avro is Snappy, so you might need to declare the compression</font>\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Transform/Convert data-files at /user/cloudera/problem5/avro-snappy and store the converted file at the following locations and file formats\n",
    "<ol>\n",
    "<li>save the data to hdfs using no compression as json file at /user/cloudera/problem5/json-no-compress</li>\n",
    "<li>save the data to hdfs using gzip compression as json file at /user/cloudera/problem5/json-gzip</li>\n",
    "</ol>\n",
    "\n",
    "<font color=green>**1. save the data to hdfs using no compression as json file at /user/cloudera/problem5/json-no-compress**</font>\n",
    "<br>  \n",
    "`prod_avro.write.format(\"json\").option(\"compression\",\"uncompressed\").option(\"mode\",\"PERMISSIVE\").save(\"/user/cca-problem/problem5/json-ext/\")`\n",
    "<br>  \n",
    "\n",
    "<font color=green>**2. save the data to hdfs using gzip compression as json file at /user/cloudera/problem5/json-gzip**</font>\n",
    "<br>  \n",
    "`prod_avro.write.format(\"json\").option(\"compression\",\"gzip\").option(\"mode\",\"PERMISSIVE\").save(\"/user/cca-problem/problem5/json-gzip/\")`\n",
    "\n",
    "`prod_json_gzip = spark.read.format(\"json\").load(\"/user/cca-problem/problem5/json-gzip/part-*\")`\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Transform/Convert data-files at  /user/cloudera/problem5/json-gzip and store the converted file at the following locations and file formats\n",
    "\n",
    "<font color=green>**1. save the data to as comma separated text using gzip compression at   /user/cloudera/problem5/csv-gzip**</font>\n",
    "<br>  \n",
    "`prod_json_gzip.write.format(\"csv\").option(\"compression\",\"gzip\").option(\"mode\",\"overwrite\").save(\"/user/cca-problem/problem5/csv-gzip/\")`\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Using spark access data at /user/cloudera/problem5/sequence and stored it back to hdfs using no compression as ORC file to HDFS to destination /user/cloudera/problem5/orc \n",
    "\n",
    "<font color=blue>**i came up with below as a solution, but i am not sure if its correct answer**</font>\n",
    "<br>  \n",
    "`prod_seq.write.format(\"orc\").option(\"compression\",\"uncompressed\").save(\"/user/cca-problem/problem5/orc-file/\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**End of the Notebook**</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
